# Real-Time llmswap Demo Session

## Developer Workflow: Multi-Provider AI for DevOps

```bash
$ llmswap providers
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Provider    â”‚ Status   â”‚ Model                â”‚ Issue               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ANTHROPIC   â”‚ âœ“ Ready  â”‚ claude-3-5-sonnet    â”‚                     â”‚
â”‚ OPENAI      â”‚ âœ“ Ready  â”‚ gpt-4o               â”‚                     â”‚
â”‚ GEMINI      â”‚ âœ“ Ready  â”‚ gemini-1.5-pro       â”‚                     â”‚
â”‚ GROQ        â”‚ âœ“ Ready  â”‚ llama-3.3-70b        â”‚                     â”‚
â”‚ OLLAMA      â”‚ âœ“ Ready  â”‚ llama3.1             â”‚                     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

$ llmswap chat
ğŸ¤– Starting chat with claude-3-5-sonnet (Tokens: 0, Cost: $0.00)

You: I'm debugging a Kubernetes deployment that's failing. The pods keep restarting.

Claude: I can help you troubleshoot the pod restart issue. Let's start by gathering some information:

1. First, let's check the pod status and recent events:
   ```bash
   kubectl get pods -o wide
   kubectl describe pod <pod-name>
   kubectl logs <pod-name> --previous
   ```

2. Common causes for pod restarts include:
   - Memory/CPU limits exceeded (OOMKilled)
   - Failed health checks
   - Application crashes
   - Configuration issues

What's the current status when you run `kubectl get pods`? Are you seeing any specific error messages?

You: Here's what I see:
```
NAME                    READY   STATUS             RESTARTS   AGE
web-app-7d4b8c6f9-x2k8r 0/1     CrashLoopBackOff   5          8m
```

The logs show: "Error: ENOENT: no such file or directory, open '/app/config.json'"

Claude: Perfect! This is a common issue. The error indicates your application is looking for `/app/config.json` but it doesn't exist in the container. Here are a few solutions:

**Option 1: ConfigMap approach**
```bash
# Create a ConfigMap with your config
kubectl create configmap app-config --from-file=config.json=./local-config.json

# Then mount it in your deployment
```

**Option 2: Check your Docker build**
Make sure your Dockerfile includes:
```dockerfile
COPY config.json /app/config.json
```

Which approach fits your setup better? Do you have the config file locally, or should we create it as a ConfigMap?

You: /switch groq

ğŸ”„ Switched to groq (llama-3.3-70b-versatile)
Tokens: 289 â†’ 289 | Cost: $0.00 â†’ $0.00

You: Actually, let me try the ConfigMap approach. How do I mount it properly?

Groq: Here's how to mount the ConfigMap properly in your deployment:

**1. Create the ConfigMap:**
```bash
kubectl create configmap app-config --from-literal=config.json='{"port": 3000, "env": "production"}'
```

**2. Update your deployment YAML:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  template:
    spec:
      containers:
      - name: web-app
        image: your-image:latest
        volumeMounts:
        - name: config-volume
          mountPath: /app/config.json
          subPath: config.json
      volumes:
      - name: config-volume
        configMap:
          name: app-config
```

**3. Apply the changes:**
```bash
kubectl apply -f deployment.yaml
kubectl rollout restart deployment/web-app
```

The key is using `subPath: config.json` to mount just the file, not the entire directory. This prevents overwriting your `/app` folder.

Want me to help you customize this for your specific config needs?

You: Perfect! Let me try this... 

```bash
$ kubectl create configmap app-config --from-file=config.json
configmap/app-config created

$ kubectl patch deployment web-app -p '{"spec":{"template":{"spec":{"volumes":[{"name":"config-volume","configMap":{"name":"app-config"}}],"containers":[{"name":"web-app","volumeMounts":[{"name":"config-volume","mountPath":"/app/config.json","subPath":"config.json"}]}]}}}}'
deployment.apps/web-app patched

$ kubectl get pods -w
NAME                       READY   STATUS    RESTARTS   AGE
web-app-85f7d6c4b6-m9x2k   1/1     Running   0          30s
```

It's working! The pod is now running successfully.

Groq: Excellent work! ğŸ‰ Your deployment is now healthy. The ConfigMap approach solved the missing config file issue perfectly.

**Quick validation steps:**
```bash
# Verify the config is mounted correctly
kubectl exec web-app-85f7d6c4b6-m9x2k -- cat /app/config.json

# Check application logs to ensure it's starting properly  
kubectl logs web-app-85f7d6c4b6-m9x2k
```

**Pro tip:** For future deployments, consider using tools like Helm or Kustomize to manage ConfigMaps and volume mounts more systematically. This pattern of "missing file â†’ ConfigMap mount" is super common in Kubernetes.

Want to explore any other aspects of your deployment, or shall we look at monitoring/alerting to catch issues like this earlier?

You: /switch openai

ğŸ”„ Switched to openai (gpt-4o)
Tokens: 289 â†’ 634 | Cost: $0.00 â†’ $0.02

You: Let's generate a monitoring alert for this type of issue

OpenAI: Great idea! Here's a comprehensive monitoring setup to catch similar issues early:

**1. Prometheus Alert for CrashLoopBackOff:**
```yaml
# alerts.yaml
groups:
- name: kubernetes-pods
  rules:
  - alert: PodCrashLoopBackOff
    expr: |
      kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"} > 0
    for: 5m
    labels:
      severity: warning
      team: platform
    annotations:
      summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is in CrashLoopBackOff"
      description: |
        Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in CrashLoopBackOff for more than 5 minutes.
        This usually indicates:
        - Missing configuration files
        - Application startup failures  
        - Resource constraints
        
        Check: kubectl logs {{ $labels.pod }} -n {{ $labels.namespace }} --previous
```

**2. Log-based Alert for Missing Files:**
```yaml
  - alert: MissingConfigurationFile
    expr: |
      increase(log_messages_total{level="error", message=~".*ENOENT.*config.*"}[5m]) > 0
    labels:
      severity: critical
      team: development
    annotations:
      summary: "Application missing configuration file"
      description: "Application is failing due to missing configuration files"
```

**3. Quick kubectl alias for debugging:**
```bash
# Add to ~/.bashrc or ~/.zshrc
alias k-debug='kubectl get pods --all-namespaces | grep -E "(Error|CrashLoop|ImagePull)" && echo "=== Recent Events ===" && kubectl get events --sort-by=.metadata.creationTimestamp --all-namespaces | tail -10'
```

Want me to help set up the Prometheus configuration, or would you prefer a different monitoring solution like Datadog/New Relic?

You: /stats

ğŸ“Š Session Statistics:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Messages: 12
Providers used: claude-3-5-sonnet, llama-3.3-70b-versatile, gpt-4o
Total tokens: 1,247 (634 input, 613 output)
Total cost: $0.02
Session duration: 15m 32s
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Provider breakdown:
â€¢ claude-3-5-sonnet: 289 tokens, $0.00
â€¢ llama-3.3-70b-versatile: 345 tokens, $0.00  
â€¢ gpt-4o: 613 tokens, $0.02

You: This is exactly what I needed! Can you help me generate a shell script that automates the ConfigMap creation for new deployments?

You: /switch anthropic

ğŸ”„ Switched to anthropic (claude-3-5-sonnet-20241220)
Tokens: 1,247 â†’ 1,247 | Cost: $0.02 â†’ $0.02

Claude: Absolutely! Here's a comprehensive shell script for automating ConfigMap deployments:

```bash
#!/bin/bash
# deploy-with-configmap.sh - Automated ConfigMap deployment script

set -euo pipefail

APP_NAME=${1:-}
CONFIG_FILE=${2:-}
NAMESPACE=${3:-default}

usage() {
    echo "Usage: $0 <app-name> <config-file> [namespace]"
    echo "Example: $0 web-app ./config.json production"
    exit 1
}

[[ -z "$APP_NAME" || -z "$CONFIG_FILE" ]] && usage

echo "ğŸš€ Deploying $APP_NAME with ConfigMap..."

# Validate config file exists
[[ ! -f "$CONFIG_FILE" ]] && echo "âŒ Config file not found: $CONFIG_FILE" && exit 1

# Create ConfigMap
echo "ğŸ“¦ Creating ConfigMap: ${APP_NAME}-config"
kubectl create configmap "${APP_NAME}-config" \
    --from-file="config.json=$CONFIG_FILE" \
    --namespace="$NAMESPACE" \
    --dry-run=client -o yaml | kubectl apply -f -

# Update deployment with ConfigMap mount
echo "ğŸ”§ Updating deployment with ConfigMap mount..."
kubectl patch deployment "$APP_NAME" \
    --namespace="$NAMESPACE" \
    --type='json' \
    -p='[
        {
            "op": "add",
            "path": "/spec/template/spec/volumes",
            "value": [{
                "name": "config-volume",
                "configMap": {"name": "'${APP_NAME}'-config"}
            }]
        },
        {
            "op": "add", 
            "path": "/spec/template/spec/containers/0/volumeMounts",
            "value": [{
                "name": "config-volume",
                "mountPath": "/app/config.json",
                "subPath": "config.json"
            }]
        }
    ]'

# Wait for rollout
echo "â³ Waiting for rollout to complete..."
kubectl rollout status deployment "$APP_NAME" --namespace="$NAMESPACE" --timeout=300s

# Verify deployment
echo "âœ… Verifying deployment..."
kubectl get pods -l app="$APP_NAME" --namespace="$NAMESPACE"

echo "ğŸ‰ Deployment complete! ConfigMap mounted successfully."
```

**Usage examples:**
```bash
# Deploy web-app with config.json in default namespace
./deploy-with-configmap.sh web-app ./config.json

# Deploy to specific namespace
./deploy-with-configmap.sh api-server ./api-config.json production

# Make it executable
chmod +x deploy-with-configmap.sh
```

This script handles error checking, creates the ConfigMap, patches the deployment, and verifies success. Want me to add any other automation features?

You: Perfect! Now let me test the cost comparison feature

```bash
$ llmswap compare --input-tokens 2000 --output-tokens 1000

Provider Cost Comparison (2000 input, 1000 output tokens):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Provider       â”‚ Cost      â”‚ Model               â”‚ Savings vs Most Expensive
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Ollama         â”‚ $0.0000   â”‚ llama3.1           â”‚ 100.0%
Groq           â”‚ $0.0002   â”‚ llama-3.3-70b      â”‚ 99.4%
Gemini         â”‚ $0.0038   â”‚ gemini-1.5-pro     â”‚ 88.1%
Anthropic      â”‚ $0.0240   â”‚ claude-3-5-sonnet  â”‚ 24.8%
OpenAI         â”‚ $0.0320   â”‚ gpt-4o             â”‚ 0.0%
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Monthly projection (100 requests/day):
â€¢ Ollama: $0.00/month (Local processing)
â€¢ Groq: $0.60/month (Lightning fast)
â€¢ Gemini: $11.40/month (Google's latest)
â€¢ Anthropic: $72.00/month (Reasoning excellence)  
â€¢ OpenAI: $96.00/month (Industry standard)

ğŸ’¡ Recommendation: Use Groq for speed-critical tasks, Claude for complex reasoning
```

Claude: Excellent demonstration of the cost comparison feature! This shows exactly why llmswap is so valuable - you can make informed decisions about provider selection based on both cost and performance needs.

**A few advanced cost optimization strategies you could implement:**

```bash
# 1. Set cost budgets per provider
llmswap config set budget.daily.openai 5.00
llmswap config set budget.monthly.anthropic 50.00

# 2. Auto-fallback to cheaper providers
llmswap config set fallback.strategy cost-optimized
llmswap config set fallback.max-cost 0.01

# 3. Smart routing based on query complexity
llmswap ask "What's 2+2?" --auto-route  # Routes to Groq (fast + cheap)
llmswap ask "Explain quantum computing" --auto-route  # Routes to Claude (complex reasoning)
```

The monthly projections are particularly useful for budget planning. Teams can now make data-driven decisions about which providers to use for different types of workloads.

You: /usage --days 30

ğŸ“Š Usage Analytics (Last 30 days):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Provider    â”‚ Requests â”‚ Tokens     â”‚ Cost    â”‚ Avg Response Time â”‚ Success Rate
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
OpenAI      â”‚ 142      â”‚ 245,231    â”‚ $18.25  â”‚ 2.3s             â”‚ 98.6%
Anthropic   â”‚ 89       â”‚ 198,452    â”‚ $15.87  â”‚ 3.1s             â”‚ 99.1%
Gemini      â”‚ 203      â”‚ 567,891    â”‚ $12.45  â”‚ 1.8s             â”‚ 97.5%
Groq        â”‚ 445      â”‚ 1,234,567  â”‚ $1.23   â”‚ 0.4s             â”‚ 99.8%
Ollama      â”‚ 67       â”‚ 89,234     â”‚ $0.00   â”‚ 8.2s             â”‚ 95.2%
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total       â”‚ 946      â”‚ 2,335,375  â”‚ $47.80  â”‚ 2.1s avg        â”‚ 98.4%

ğŸ¯ Insights:
â€¢ Groq handling 47% of requests (speed advantage)
â€¢ Claude highest success rate at 99.1%
â€¢ Ollama for offline/privacy-sensitive tasks
â€¢ $47.80 total spend vs $156.80 single-provider equivalent (69% savings)

ğŸ’° Cost Optimization Opportunities:
â€¢ Route simple queries to Groq (potential $8.50/month savings)
â€¢ Use Gemini for content generation (25% faster than GPT-4)
â€¢ Consider Ollama for development/testing workloads

You: /quit

ğŸ‘‹ Session complete! 

ğŸ“Š Final Statistics:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Messages: 18
Providers used: 4 (claude, groq, openai, anthropic)
Total tokens: 3,089
Total cost: $0.08
Session duration: 28m 15s
Conversation memory: Maintained across all providers âœ“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ This session demonstrated:
â€¢ Multi-provider debugging workflow
â€¢ Real-time provider switching  
â€¢ Cost-aware decision making
â€¢ Comprehensive usage analytics
â€¢ Persistent conversation context

Thanks for using llmswap! ğŸš€
```